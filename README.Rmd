---
output: github_document
always_allow_html: true
bibliography: README.bib 
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "80%",
  fig.height = 4,
  fig.width = 6
)

library(magrittr)
library(kableExtra)
library(tidyverse)

devtools::load_all()
```

# rbqmR

<!-- badges: start -->
[![CRAN status](https://www.r-pkg.org/badges/version/rbqmR)](https://CRAN.R-project.org/package=rbqmR)
<a href="https://www.repostatus.org/#wip"><img src="https://www.repostatus.org/badges/latest/wip.svg" alt="Project Status: WIP â€“ Initial development is in progress, but there has not yet been a stable, usable release suitable for the public." /></a>
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![R-CMD-check](https://github.com/openpharma/rbqmR/workflows/R-CMD-check/badge.svg)](https://github.com/openpharma/rbqmR/actions)
<!-- badges: end -->

## Introduction
The purpose of the `rbqmR` package is to provide a repository of r-based tools for the implementation of risk-based quality management.

Tools currently exist for

* Dynamic Quality Tolerance Limits (QTLs) using Bayesian Hierarchical Models (ongoing)
* Observed-Minus-Expected methodology (ongoing)
* Observed/Expected methodology (ongoing)

This package is a work-in-progress.  It's primary focus is dynamic QTLs. Other methodologies are included for completeness.

## Installation

You can install the development version of rbqmR from [GitHub](https://github.com/) with:

```{r, eval=FALSE}
# install.packages("devtools")
devtools::install_github("openpharma/rbqmR")
```

## Examples
### Dynamic QTLs

We use example 2.7 of Berry et al [@BERRY], described on pages 52 to 63, modifying the context so that rather than being a meta analysis of several different trials, we consider the data to represent the performance of different sites within a single trial.  The exact metric being measured is immaterial, though it remains a summary of a binomial outcome.

```{r}
data(berrySummary)

berrySummary %>% kable(digits = c(0, 0, 0, 2))
```
The central tenet of the QTL methodology implemented in `rbqmR` is not that a current trial should behave in a similar fashion to a "similar" historical or control trial, but rather that all sites within the current trial should behave in a similar way.  The justification for this assumption is that the trial's inclusion/exclusion criteria are designed to minimise heterogeneity amongst the study population (save for that induced by differences in treatment in comparative trials).  

We fit the Bayesian Hierarchical Model described by Berry et al ...

```{r, error=TRUE}
fitted <- berrySummary %>%
  fitBayesBinomialModel(n = Subjects, r = Events)
```


... and use the quantiles of the posterior distribution of the probability of an event to define the QTLs for this metric.  This can be done in isolation (when the trial acts as its own control) or with reference to historical data obtained from similar previous studies.

#### Without historical data
For the sake of argument, suppose we set the lower and upper QTLs to be the 5th and 95th centiles of the posterior respectively.  Since we have nine sites, it's not unreasonable to expect one site to be outside this range.  So we define a breach of the QTL to have occurred when the observed event rates at two or more sites fall outside this range.

The 5th and 95th centiles of the posterior are 
```{r, error=TRUE}
quantiles <- fitted$tab %>%
  summarise(
    Q05 = quantile(p, probs = 0.05, names = FALSE),
    Q95 = quantile(p, probs = 0.95, names = FALSE)
  )
quantiles
```
So, in this specific case, our QTLs translate to observed event rates of `r sprintf("%5.2f%%", 100*quantiles$Q05)` and `r sprintf("%5.2f%%", 100*quantiles$Q95)` respectively.

Do any sites have observed event rates outside this range?

```{r}
berrySummary %>%
  applyQtl(
    var = ObservedResponse,
    lower = quantiles %>% pull(Q05),
    upper = quantiles %>% pull(Q95)
  ) %>%
  kable(
    digits = c(0, 0, 0, 3, 3, 3),
    caption = "Sites breaching the QTL"
  )
```

Yes.  Two sites are outside this range.  One above and one below.  The QTL has been breached.  The process can be summarised graphically.

```{r}
fitted$tab %>%
  createQtlPlot(
    actionLimits = list(
      list("lower" = quantiles$Q95, "upper" = NA, "alpha" = 0.3, "colour" = "goldenrod1"),
      list("lower" = NA, "upper" = quantiles$Q05, "alpha" = 0.3, "colour" = "goldenrod1")
    ),
    siteData = berrySummary,
    siteSize = Subjects,
    siteMetric = ObservedResponse
  )
```

#### With historical data
Suppose previous experience tells us that the event probability in this type of study should be between 0.50 and 0.75.  We define the QTL such that we require the posterior event probability for a new participant to be in the range 0.5 to 0.75 inclusive to be at least 50%.

> When using a Bayesian Hierarchical Model, the probabilities associated with credible intervals are generally lower than those associated with similar frequentist models.  This is because BHMs permit  more sources of variation.  Here, the BHM permits variation between the response rates at different sites, even when considering the overall event rate for the study.  The corresponding frequentist analysis assumes that all sites share  common event rate, thus assuming there is no inter-site variation.

```{r}
fitted$tab %>%
  summarise(PosteriorProb = mean(p >= 0.5 & p <= 0.75))
```
Again, the QTL is breached, and the process can be summarised graphically.

```{r}
fitted$tab %>%
  createQtlPlot(
    targetRange = list("lower" = 0.5, "upper" = 0.75),
    observedMetric = fitted$tab %>% summarise(Mean = mean(p)) %>% pull(Mean)
  )
```

The site-level KRIs can be added to the plot to help focus attention where intervention is likely to have the largest effect.

```{r}
fitted$tab %>%
  createQtlPlot(
    targetRange = list("lower" = 0.5, "upper" = 0.75),
    observedMetric = fitted$tab %>% summarise(Mean = mean(p)) %>% pull(Mean),
    siteData = berrySummary,
    siteSize = Subjects,
    siteMetric = ObservedResponse,
    actionLimits = list(
      list("lower" = quantiles$Q95, "upper" = NA, "alpha" = 0.3, "colour" = "goldenrod1"),
      list("lower" = NA, "upper" = quantiles$Q05, "alpha" = 0.3, "colour" = "goldenrod1")
    )
  )
```

#### Built-in functions to assist in the evaluation of QTLs
For illustration, in this section we continue to use the example provided by Berry et al. 

##### By comparing the derived metric to a constant value
The `evaluatePointEstimateQTL` allows the comparison of an arbitrary scalar summary statistic derived from the estimate of the posterior distribution (which defaults to the mean), with an arbitrary number of lower and upper limits.

For example, the code below defines a QTL based on the mean of the posterior distribution of the probability of an event.  Call this probability $\hat{p}$.  The warning limits are 0.1 and 0.7.  The action limits are 0.2 and 0.9.
```{r}
berrySummary %>%
  evaluatePointEstimateQTL(
    posterior = fitted$tab,
    metric = p,
    observedMetric = ObservedResponse,
    lower = c("warn" = 0.1, "action" = 0.2),
    upper = c("warn" = 0.7, "action" = 0.9)
  )
```
As with all `evaluateXXXXQTL` functions, the return value of `evaluatePointEstimateQTL` is a list.  The `status` element indicates whether or not the QTL's limits have been breached.  The `qtl` element gives the calculated value of the QTL metric and the `data` element returns a copy of the `data.frame` containing the site level KRIs augmented with a column indicating which, if any, of the various limits were breached by that site.

Both the `lower` and `upper` parameters are optional (though at least one must be given) and the number of limits, and their labels, are arbitrary.

```{r, eval=FALSE}
berrySummary %>%
  evaluatePointEstimateQTL(
    posterior = fitted$tab,
    metric = p,
    observedMetric = ObservedResponse,
    upper = c("mild" = 0.6, "moderate" = 0.8, "severe" = 0.9)
  )
```

If only one limit is defined, this can be provided as a scalar, in which case it is labelled `action`.

The function on which the QTL is based is specified by the `stat` parameter of `evaluatePointEstimateQTL` and can be a user-defined function.  For example, the following code fragments define QTLs based on the median

```{r}
berrySummary %>%
  evaluatePointEstimateQTL(
    posterior = fitted$tab,
    metric = p,
    stat = median,
    observedMetric = ObservedResponse,
    upper = c("warn" = 0.7, "action" = 0.9)
  )
```

and 10th centile of the posterior distribution of $\hat{p}$.

```{r}
berrySummary %>%
  evaluatePointEstimateQTL(
    posterior = fitted$tab,
    metric = p,
    stat = function(x) quantile(x, probs = 0.1),
    observedMetric = ObservedResponse,
    upper = c("warn" = 0.3, "action" = 0.8)
  )
```

##### By calculating the probability that the derived metric is in a given range

> TO DO

##### By using an arbitrary criterion

`evaluatePointEstimateQTL`, `evaluateXXXXQTL` and `evaluateXXXXQTL` are wrappers around `evaluateCustomQTL`, which can be used to evaluate an arbitrary, user-defined QTL rule. `evaluateCustomQTL` takes the following parameters:

* `data`: a tibble containing site-level observed metrics
* `posterior`: a tibble containing the posterior distribution of the metric, usually obtained from a fit Bayes model function.
* `f`: a function whose first two parameters are `data` and `posterior`, in that ordered and with those names
* `statusCol=Status`:
* `...`: additional parameters passed to `f`.

Essentially, all that `evaluateCustomQTL` does is to perform some basic checks on its parameter values and then return the value returned by `data %>% f(posterior, ...)`.  So, for example, a simplified version of `evaluatePointEstimateQTL` that compares $\hat{p}$ to 0.6 might be

```{r, error=TRUE}
berrySummary %>%
  evaluateCustomQTL(
    posterior = fitted$tab,
    f = function(data, posterior) {
      rv <- list()
      rv$qtl <- posterior %>%
        summarise(qtl = mean(p)) %>%
        pull(qtl)
      rv$status <- ifelse(rv$qtl < 0.6, "OK", "Breach")
      rv$data <- data %>% mutate(Status = ifelse(ObservedResponse < 0.6, "OK", "Breach"))
      rv
    }
  )
```

### Observed - Expected Methodology
We generate some random data similar to that used by Gilbert [@GILBERT], after setting a seed for reproducibility.

In order to illustrate what happens when a QTL is breached, we set the probability that a participant reports an event to 0.13, even though the QTL process will assume the event rate is 0.10...

```{r}
set.seed(011327)

randomData <- tibble(
  Subject = 1:400,
  Event = rbinom(400, 1, 0.13)
)
```

... and create an observed-expected table ...

```{r}
omeTable <- randomData %>%
  createObservedMinusExpectedTable(
    timeVar = Subject,
    eventVar = Event,
    eventArray = 1,
    expectedRate = 0.1,
    maxTrialSize = 400
  )
```

... and plot the corresponding graph. 

```{r}
omeTable %>%
  createObservedMinusExpectedPlot()
```

We can see that the trial breached a warning limit.  When did this first happen?

```{r}
omeTable %>%
  filter(Status != "OK") %>%
  head(1) %>%
  select(-contains("Action"), -SubjectIndex) %>%
  kable(
    col.names = c("Subject", "Event", "Cumulative Events", "O - E", "Status", "Lower", "Upper"),
    caption = "First breach of an action or warning limit"
  ) %>%
  add_header_above(c(" " = 5, "Warning Limits" = 2))
```

### Observed / Expected methodology

Katz et al [@KATZ] calculate the confidence interval for the ratio of two binomial random variables.  We use this to determine QTLs for the ratio of observed over expected proportions.  The variability associated with a ratio suggests that this methodology is likely to be useful only for large studies with low expected event rates.

We require historical trial data to implement this methodology.

Suppose we have data on 10,000 historical patients who have reported a given event at a rate of 1.4%.  We are planning a 1500 patient trial and have no reason to suppose the event rate in the trial will be any different from what has been seen in the past.

```{r}
createObservedOverExpectedTable(
  nHistorical = 10000,
  historicalRate = 0.014,
  expectedRate = 0.014,
  nObservedRange = seq(50, 1500, 25)
) %>%
  createObservedOverExpectedPlot()
```

As the trial is executed, the observed data can be added to the table and the plot.

```{r, error=TRUE}
observedData <- tibble(
  NObserved = c(250, 500, 750, 1000),
  ObservedRate = 100 * c(2, 9, 15, 16) / NObserved
)

table <- createObservedOverExpectedTable(
  nHistorical = 10000,
  historicalRate = 0.014,
  expectedRate = 0.014,
  nObservedRange = seq(50, 1500, 25),
  observedData = observedData
)

table %>% createObservedOverExpectedPlot(observedRate = ObservedRate)
```

## Beyond TransCelerate
At the time of writing (late 2022) The TransCelerate Quality Tolerance Limit Framework [@Transcelerate2020] lists metrics that are exclusively binary in nature.  There are many other potential metrics that are non-binary and which may provide insight into the conduct of the trial.  For example,

* The number of episodes of rescue medication (as opposed to the percentage or number of trial participants on rescue medication)
* Time to withdrawal of consent (as opposed to the percentage or number of trial participants with withdrawal of informed consent)

As well as other metrics that can't easily be dichotomised

* Drug plasma levels
* Number of (S)AEs reported per time unit of drug exposure
* Time to respond to data queries

The Bayesian QTL framework implemented in `rbqmR` can easily be extended to include these other data types.

### Events per unit time
We use data on the numbers of Prussian cavalry officers kicked to death by horses [@Bortkiewicz1898] to illustrate the method.
```{r}
data("cavalryDeaths")

cavalryDeaths
```

Regard different cavalry Corps as "sites" and regard the number of years for which data were collected as "exposure".

```{r}

cavalrySummary <- cavalryDeaths %>%
  group_by(Corps) %>%
  summarise(
    Deaths = sum(Deaths),
    TotalTime = n(),
    .groups = "drop"
  ) %>%
  mutate(DeathRate = Deaths / TotalTime)

cavalrySummary
```

Although not necessary here, because data for all Corps was recorded for the same amount of time, the Poisson model used by `rbqmR` adjusts risk by total exposure at the site.

```{r, results="as-is"}
getModelString("poisson")
```
Fitting the model is straightforward.

```{r}
poissonFit <- cavalrySummary %>%
  fitBayesPoissonModel(Deaths, TotalTime)
poissonFit$tab %>%
  createQtlPlot(
    metric = lambda,
    siteData = cavalrySummary,
    siteSize = TotalTime,
    siteMetric = DeathRate
  ) +
  labs(x = "Deaths per year")
```

## References
